{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pys import credentials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing interactive authentication. Please follow the instructions on the terminal.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default web browser has been opened at https://login.microsoftonline.com/organizations/oauth2/v2.0/authorize. Please continue the login in the web browser. If no web browser is available or if the web browser fails to open, use device code flow with `az login --use-device-code`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive authentication successfully completed.\n",
      "Deploying StorageAccount with name handmadestorage822b6e018.\n",
      "Deploying AppInsights with name handmadeinsights4f9e508a.\n",
      "Deployed AppInsights with name handmadeinsights4f9e508a. Took 4.13 seconds.\n",
      "Deploying KeyVault with name handmadekeyvault0f212c84.\n",
      "Deployed KeyVault with name handmadekeyvault0f212c84. Took 18.55 seconds.\n",
      "Deploying Workspace with name handmade_ntt_data_challenge.\n",
      "Deployed StorageAccount with name handmadestorage822b6e018. Took 23.62 seconds.\n",
      "Deployed Workspace with name handmade_ntt_data_challenge. Took 35.61 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Create Workspace\n",
    "\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.create(name                  = \"handmade_ntt_data_challenge\",\n",
    "                      subscription_id       = credentials.azure_subscription_id,\n",
    "                      resource_group        = credentials.azure_resource_group,\n",
    "                      create_resource_group = False,\n",
    "                      location              = credentials.azure_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Workspace\n",
    "\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace(workspace_name        = \"handmade_ntt_data_challenge\",\n",
    "               subscription_id       = credentials.azure_subscription_id,\n",
    "               resource_group        = credentials.azure_resource_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model handmade_model\n"
     ]
    }
   ],
   "source": [
    "# Register the model\n",
    "\n",
    "from azureml.core.model import Model\n",
    "\n",
    "model = Model.register(workspace  = ws, \n",
    "                       model_path = \"../resources/model.pkl\", \n",
    "                       model_name = \"handmade_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model handmade_x_scaler\n"
     ]
    }
   ],
   "source": [
    "# Register the x_scaler\n",
    "\n",
    "from azureml.core.model import Model\n",
    "\n",
    "model = Model.register(workspace  = ws, \n",
    "                       model_path = \"../resources/x_scaler.pkl\", \n",
    "                       model_name = \"handmade_x_scaler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define inference config\n",
    "\n",
    "from azureml.core import Environment\n",
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "env = Environment(name = \"handmade_env\")\n",
    "dummy_inference_config = InferenceConfig(environment      = env,\n",
    "                                         source_directory = \"../pys/\",\n",
    "                                         entry_script     = \"../pys/score.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local deployment\n",
    "\n",
    "from azureml.core.webservice import LocalWebservice\n",
    "\n",
    "deployment_config = LocalWebservice.deploy_configuration(port=6789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/10/5nbnv2696vlgb605wf3g6wwh0000gn/T/ipykernel_2419/428816697.py:1: FutureWarning: azureml.core.model:\n",
      "To leverage new model deployment capabilities, AzureML recommends using CLI/SDK v2 to deploy models as online endpoint, \n",
      "please refer to respective documentations \n",
      "https://docs.microsoft.com/azure/machine-learning/how-to-deploy-managed-online-endpoints /\n",
      "https://docs.microsoft.com/azure/machine-learning/how-to-attach-kubernetes-anywhere \n",
      "For more information on migration, see https://aka.ms/acimoemigration \n",
      "To disable CLI/SDK v1 deprecation warning set AZUREML_LOG_DEPRECATION_WARNING_ENABLED to 'False'\n",
      "  service = Model.deploy(workspace         = ws,\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Models must either be of type azureml.core.model.Model or a str path to a file or folder.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/10/5nbnv2696vlgb605wf3g6wwh0000gn/T/ipykernel_2419/428816697.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m service = Model.deploy(workspace         = ws,\n\u001b[0m\u001b[1;32m      2\u001b[0m                        \u001b[0mname\u001b[0m              \u001b[0;34m=\u001b[0m \u001b[0;34m\"handmade-predict\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                        \u001b[0mmodels\u001b[0m            \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                        \u001b[0minference_config\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mdummy_inference_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                        \u001b[0mdeployment_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeployment_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/azureml/core/model.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(workspace, name, models, inference_config, deployment_config, deployment_target, overwrite, show_output)\u001b[0m\n\u001b[1;32m   1661\u001b[0m         \u001b[0;31m# Local webservice.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1662\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeployment_config\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeployment_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLocalWebserviceDeploymentConfiguration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1663\u001b[0;31m             return deployment_config._webservice_type._deploy(workspace, name, models,\n\u001b[0m\u001b[1;32m   1664\u001b[0m                                                               \u001b[0minference_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minference_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1665\u001b[0m                                                               deployment_config=deployment_config)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/azureml/core/webservice/local.py\u001b[0m in \u001b[0;36m_deploy\u001b[0;34m(workspace, name, models, image_config, deployment_config, wait, inference_config)\u001b[0m\n\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0mservice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLocalWebservice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmust_exist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m         service.update(models=models,\n\u001b[0m\u001b[1;32m    740\u001b[0m                        \u001b[0mimage_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m                        \u001b[0minference_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minference_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/azureml/core/webservice/local.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                 raise WebserviceException('Cannot call {}() when service is {}.'.format(func.__name__, self.state),\n\u001b[1;32m     71\u001b[0m                                           logger=module_logger)\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/azureml/core/webservice/local.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, models, image_config, deployment_config, wait, inference_config)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_models_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_models_local\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_models_remote\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLocalWebservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_identify_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimage_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/azureml/core/webservice/local.py\u001b[0m in \u001b[0;36m_identify_models\u001b[0;34m(workspace, models)\u001b[0m\n\u001b[1;32m    927\u001b[0m                 \u001b[0mremote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m                 raise NotImplementedError('Models must either be of type azureml.core.model.Model or a str '\n\u001b[0m\u001b[1;32m    930\u001b[0m                                           'path to a file or folder.')\n\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Models must either be of type azureml.core.model.Model or a str path to a file or folder."
     ]
    }
   ],
   "source": [
    "service = Model.deploy(workspace         = ws,\n",
    "                       name              = \"handmade-predict\",\n",
    "                       models            = [model],\n",
    "                       inference_config  = dummy_inference_config,\n",
    "                       deployment_config = deployment_config,\n",
    "                       overwrite         = True)\n",
    "\n",
    "service.wait_for_deployment(show_output = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud deployment\n",
    "\n",
    "env = Environment(name='cloudenv')\n",
    "python_packages = ['joblib', 'numpy']\n",
    "for package in python_packages:\n",
    "    env.python.conda_dependencies.add_pip_package(package)\n",
    "\n",
    "inference_config = InferenceConfig(environment=env, source_directory='../pys/', entry_script='../pys/score.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/10/5nbnv2696vlgb605wf3g6wwh0000gn/T/ipykernel_2419/3138792519.py:3: FutureWarning: azureml.core.model:\n",
      "To leverage new model deployment capabilities, AzureML recommends using CLI/SDK v2 to deploy models as online endpoint, \n",
      "please refer to respective documentations \n",
      "https://docs.microsoft.com/azure/machine-learning/how-to-deploy-managed-online-endpoints /\n",
      "https://docs.microsoft.com/azure/machine-learning/how-to-attach-kubernetes-anywhere \n",
      "For more information on migration, see https://aka.ms/acimoemigration \n",
      "To disable CLI/SDK v1 deprecation warning set AZUREML_LOG_DEPRECATION_WARNING_ENABLED to 'False'\n",
      "  service = Model.deploy(workspace         = ws,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model handmade_model:2 to /var/folders/10/5nbnv2696vlgb605wf3g6wwh0000gn/T/azureml_9hpu2rbo/handmade_model/2\n",
      "Generating Docker build context.\n",
      "Package creation Succeeded\n",
      "Logging into Docker registry e4e0e96ebb3f40cdb53da54690e88f94.azurecr.io\n",
      "Logging into Docker registry e4e0e96ebb3f40cdb53da54690e88f94.azurecr.io\n",
      "Building Docker image from Dockerfile...\n",
      "Step 1/5 : FROM e4e0e96ebb3f40cdb53da54690e88f94.azurecr.io/azureml/azureml_de295a9a9e47e09a199d4bf845dd6bf2\n",
      " ---> 55c9e0ac853c\n",
      "Step 2/5 : COPY azureml-app /var/azureml-app\n",
      " ---> 3caee378ef76\n",
      "Step 3/5 : RUN mkdir -p '/var/azureml-app' && echo eyJhY2NvdW50Q29udGV4dCI6eyJzdWJzY3JpcHRpb25JZCI6IjQ2ZmE4NTZiLTgyYzgtNGIzNy04MzM5LTk2ZDBmMzg1OTljNyIsInJlc291cmNlR3JvdXBOYW1lIjoiZ3UxIiwiYWNjb3VudE5hbWUiOiJoYW5kbWFkZV9udHRfZGF0YV9jaGFsbGVuZ2UiLCJ3b3Jrc3BhY2VJZCI6ImU0ZTBlOTZlLWJiM2YtNDBjZC1iNTNkLWE1NDY5MGU4OGY5NCJ9LCJtb2RlbHMiOnt9LCJtb2RlbHNJbmZvIjp7fX0= | base64 --decode > /var/azureml-app/model_config_map.json\n",
      " ---> [Warning] The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\n",
      " ---> Running in 262362d9d610\n",
      " ---> 877ebb4e5fb0\n",
      "Step 4/5 : RUN mv '/var/azureml-app/tmphhgxdbpt.py' /var/azureml-app/main.py\n",
      " ---> [Warning] The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\n",
      " ---> Running in f9feeb364169\n",
      " ---> 5c0986dcece1\n",
      "Step 5/5 : CMD [\"runsvdir\",\"/var/runit\"]\n",
      " ---> [Warning] The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\n",
      " ---> Running in aeabb11f3626\n",
      " ---> 43a050a6ccab\n",
      "Successfully built 43a050a6ccab\n",
      "Successfully tagged handmade-predict:latest\n",
      "Container (name:relaxed_euclid, id:cafcd061181685060d0342e6e0e73f1131ddbb47d0453ba897ff7d1d762f573e) cannot be killed.\n",
      "Container has been successfully cleaned up.\n",
      "Image sha256:c41cbad938661403862e24e81fe39db4ac28e886e024e9e1a369ccdb3f27fd5d successfully removed.\n",
      "Starting Docker container...\n",
      "Docker container running.\n",
      "Checking container health...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: Container has crashed. Did your init method fail?\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Container Logs:\n",
      "/bin/bash: /azureml-envs/azureml_f3f7e6c5fb83d94df23933000bf02da3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "/bin/bash: /azureml-envs/azureml_f3f7e6c5fb83d94df23933000bf02da3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "/bin/bash: /azureml-envs/azureml_f3f7e6c5fb83d94df23933000bf02da3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "/bin/bash: /azureml-envs/azureml_f3f7e6c5fb83d94df23933000bf02da3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "2023-03-27T16:47:10,816470418+00:00 - rsyslog/run \n",
      "2023-03-27T16:47:10,865334085+00:00 - iot-server/run \n",
      "bash: /azureml-envs/azureml_f3f7e6c5fb83d94df23933000bf02da3/lib/libtinfo.so.6: no version information available (required by bash)\n",
      "2023-03-27T16:47:10,923213377+00:00 - nginx/run \n",
      "2023-03-27T16:47:10,976290210+00:00 - gunicorn/run \n",
      "2023-03-27T16:47:11,027643543+00:00 | gunicorn/run | \n",
      "2023-03-27T16:47:11,071901710+00:00 | gunicorn/run | ###############################################\n",
      "2023-03-27T16:47:11,102735210+00:00 | gunicorn/run | AzureML Container Runtime Information\n",
      "2023-03-27T16:47:11,150242294+00:00 | gunicorn/run | ###############################################\n",
      "2023-03-27T16:47:11,188688627+00:00 | gunicorn/run | \n",
      "2023-03-27T16:47:11,236856627+00:00 | gunicorn/run | \n",
      "2023-03-27T16:47:11,301578544+00:00 | gunicorn/run | AzureML image information: openmpi4.1.0-ubuntu20.04, Materializaton Build:20230120.v2\n",
      "2023-03-27T16:47:11,333755544+00:00 | gunicorn/run | \n",
      "2023-03-27T16:47:11,375442044+00:00 | gunicorn/run | \n",
      "2023-03-27T16:47:11,425458085+00:00 | gunicorn/run | PATH environment variable: /azureml-envs/azureml_f3f7e6c5fb83d94df23933000bf02da3/bin:/opt/miniconda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n",
      "2023-03-27T16:47:11,457467627+00:00 | gunicorn/run | PYTHONPATH environment variable: \n",
      "2023-03-27T16:47:11,497380669+00:00 | gunicorn/run | \n",
      "2023-03-27T16:47:11,533216335+00:00 | gunicorn/run | Pip Dependencies (before dynamic installation)\n",
      "\n",
      "EdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n",
      "/bin/bash: /azureml-envs/azureml_f3f7e6c5fb83d94df23933000bf02da3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "2023-03-27T16:47:12,135542877+00:00 - iot-server/finish 1 0\n",
      "2023-03-27T16:47:12,176768002+00:00 - Exit code 1 is normal. Not restarting iot-server.\n",
      "adal==1.2.7\n",
      "argcomplete==2.1.2\n",
      "attrs==22.2.0\n",
      "azure-common==1.1.28\n",
      "azure-core==1.26.3\n",
      "azure-graphrbac==0.61.1\n",
      "azure-identity==1.12.0\n",
      "azure-mgmt-authorization==3.0.0\n",
      "azure-mgmt-containerregistry==10.1.0\n",
      "azure-mgmt-core==1.3.2\n",
      "azure-mgmt-keyvault==10.2.0\n",
      "azure-mgmt-resource==21.2.1\n",
      "azure-mgmt-storage==20.1.0\n",
      "azureml-core==1.49.0\n",
      "azureml-dataprep==4.9.5\n",
      "azureml-dataprep-native==38.0.0\n",
      "azureml-dataprep-rslex==2.16.4\n",
      "azureml-dataset-runtime==1.49.0\n",
      "azureml-defaults==1.49.0\n",
      "azureml-inference-server-http==0.8.3\n",
      "backports.tempfile==1.0\n",
      "backports.weakref==1.0.post1\n",
      "bcrypt==4.0.1\n",
      "cachetools==5.3.0\n",
      "certifi @ file:///croot/certifi_1671487769961/work/certifi\n",
      "cffi==1.15.1\n",
      "charset-normalizer==3.1.0\n",
      "click==8.1.3\n",
      "cloudpickle==2.2.1\n",
      "contextlib2==21.6.0\n",
      "cryptography==40.0.1\n",
      "distro==1.8.0\n",
      "docker==6.0.1\n",
      "dotnetcore2==3.1.23\n",
      "Flask==2.2.3\n",
      "Flask-Cors==3.0.10\n",
      "fusepy==3.0.1\n",
      "google-api-core==2.11.0\n",
      "google-auth==2.16.3\n",
      "googleapis-common-protos==1.59.0\n",
      "gunicorn==20.1.0\n",
      "humanfriendly==10.0\n",
      "idna==3.4\n",
      "importlib-metadata==6.1.0\n",
      "importlib-resources==5.12.0\n",
      "inference-schema==1.5.1\n",
      "isodate==0.6.1\n",
      "itsdangerous==2.1.2\n",
      "jeepney==0.8.0\n",
      "Jinja2==3.1.2\n",
      "jmespath==1.0.1\n",
      "jsonpickle==2.2.0\n",
      "jsonschema==4.17.3\n",
      "knack==0.10.1\n",
      "MarkupSafe==2.1.2\n",
      "msal==1.21.0\n",
      "msal-extensions==1.0.0\n",
      "msrest==0.7.1\n",
      "msrestazure==0.6.4\n",
      "ndg-httpsclient==0.5.1\n",
      "numpy==1.23.5\n",
      "oauthlib==3.2.2\n",
      "opencensus==0.11.2\n",
      "opencensus-context==0.1.3\n",
      "opencensus-ext-azure==1.1.9\n",
      "packaging==21.3\n",
      "paramiko==2.12.0\n",
      "pathspec==0.11.1\n",
      "pkginfo==1.9.6\n",
      "pkgutil_resolve_name==1.3.10\n",
      "portalocker==2.7.0\n",
      "protobuf==4.22.1\n",
      "psutil==5.9.4\n",
      "pyarrow==9.0.0\n",
      "pyasn1==0.4.8\n",
      "pyasn1-modules==0.2.8\n",
      "pycparser==2.21\n",
      "pydantic==1.10.7\n",
      "Pygments==2.14.0\n",
      "PyJWT==2.6.0\n",
      "PyNaCl==1.5.0\n",
      "pyOpenSSL==23.1.0\n",
      "pyparsing==3.0.9\n",
      "pyrsistent==0.19.3\n",
      "PySocks==1.7.1\n",
      "python-dateutil==2.8.2\n",
      "pytz==2023.2\n",
      "PyYAML==6.0\n",
      "requests==2.28.2\n",
      "requests-oauthlib==1.3.1\n",
      "rsa==4.9\n",
      "SecretStorage==3.3.3\n",
      "six==1.16.0\n",
      "tabulate==0.9.0\n",
      "typing_extensions==4.5.0\n",
      "urllib3==1.26.15\n",
      "websocket-client==1.5.1\n",
      "Werkzeug==2.2.3\n",
      "wrapt==1.12.1\n",
      "zipp==3.15.0\n",
      "\n",
      "2023-03-27T16:47:15,684405546+00:00 | gunicorn/run | \n",
      "2023-03-27T16:47:15,717739962+00:00 | gunicorn/run | ###############################################\n",
      "2023-03-27T16:47:15,746499837+00:00 | gunicorn/run | AzureML Inference Server\n",
      "2023-03-27T16:47:15,778148712+00:00 | gunicorn/run | ###############################################\n",
      "2023-03-27T16:47:15,858855504+00:00 | gunicorn/run | \n",
      "2023-03-27T16:47:22,765981174+00:00 | gunicorn/run | Starting AzureML Inference Server HTTP.\n",
      "2023-03-27 16:47:24,538 I [23] azmlinfsrv - Loaded logging config from /azureml-envs/azureml_f3f7e6c5fb83d94df23933000bf02da3/lib/python3.8/site-packages/azureml_inference_server_http/logging.json\n",
      "2023-03-27 16:47:25,611 I [23] gunicorn.error - Starting gunicorn 20.1.0\n",
      "2023-03-27 16:47:25,616 I [23] gunicorn.error - Listening at: http://0.0.0.0:31311 (23)\n",
      "2023-03-27 16:47:25,617 I [23] gunicorn.error - Using worker: sync\n",
      "2023-03-27 16:47:25,626 I [173] gunicorn.error - Booting worker with pid: 173\n",
      "\n",
      "Azure ML Inferencing HTTP server v0.8.3\n",
      "\n",
      "\n",
      "Server Settings\n",
      "---------------\n",
      "Entry Script Name: /var/azureml-app/pys/score.py\n",
      "Model Directory: azureml-models/handmade_model/2\n",
      "Config File: None\n",
      "Worker Count: 1\n",
      "Worker Timeout (seconds): 300\n",
      "Server Port: 31311\n",
      "Application Insights Enabled: false\n",
      "Application Insights Key: None\n",
      "Inferencing HTTP server version: azmlinfsrv/0.8.3\n",
      "CORS for the specified origins: None\n",
      "\n",
      "\n",
      "Server Routes\n",
      "---------------\n",
      "Liveness Probe: GET   127.0.0.1:31311/\n",
      "Score:          POST  127.0.0.1:31311/score\n",
      "\n",
      "/azureml-envs/azureml_f3f7e6c5fb83d94df23933000bf02da3/lib/python3.8/site-packages/azureml_inference_server_http/server/config.py:51: FutureWarning: aliases are no longer used by BaseSettings to define which environment variables to read. Instead use the \"env\" field setting. See https://pydantic-docs.helpmanual.io/usage/settings/#environment-variable-names\n",
      "  class AMLInferenceServerConfig(pydantic.BaseSettings):\n",
      "2023-03-27 16:47:28,228 I [173] azmlinfsrv - AML_FLASK_ONE_COMPATIBILITY is set. Patched Flask to ensure compatibility with Flask 1.\n",
      "Initializing logger\n",
      "2023-03-27 16:47:28,251 I [173] azmlinfsrv - Starting up app insights client\n",
      "2023-03-27 16:47:28,264 E [173] azmlinfsrv - Traceback (most recent call last):\n",
      "  File \"/azureml-envs/azureml_f3f7e6c5fb83d94df23933000bf02da3/lib/python3.8/site-packages/azureml_inference_server_http/server/user_script.py\", line 74, in load_script\n",
      "    main_module_spec.loader.exec_module(user_module)\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/var/azureml-app/pys/score.py\", line 1, in <module>\n",
      "    import joblib\n",
      "ModuleNotFoundError: No module named 'joblib'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/azureml-envs/azureml_f3f7e6c5fb83d94df23933000bf02da3/lib/python3.8/site-packages/azureml_inference_server_http/server/aml_blueprint.py\", line 88, in setup\n",
      "    self.user_script.load_script(config.app_root)\n",
      "  File \"/azureml-envs/azureml_f3f7e6c5fb83d94df23933000bf02da3/lib/python3.8/site-packages/azureml_inference_server_http/server/user_script.py\", line 76, in load_script\n",
      "    raise UserScriptImportException(ex) from ex\n",
      "azureml_inference_server_http.server.user_script.UserScriptImportException: Failed to import user script because it raised an unhandled exception\n",
      "\n",
      "2023-03-27 16:47:28,265 I [173] gunicorn.error - Worker exiting (pid: 173)\n",
      "2023-03-27 16:47:28,501 I [23] gunicorn.error - Shutting down: Master\n",
      "2023-03-27 16:47:28,503 I [23] gunicorn.error - Reason: Worker failed to boot.\n",
      "\n",
      "Azure ML Inferencing HTTP server v0.8.3\n",
      "\n",
      "\n",
      "Server Settings\n",
      "---------------\n",
      "Entry Script Name: /var/azureml-app/pys/score.py\n",
      "Model Directory: azureml-models/handmade_model/2\n",
      "Config File: None\n",
      "Worker Count: 1\n",
      "Worker Timeout (seconds): 300\n",
      "Server Port: 31311\n",
      "Application Insights Enabled: false\n",
      "Application Insights Key: None\n",
      "Inferencing HTTP server version: azmlinfsrv/0.8.3\n",
      "CORS for the specified origins: None\n",
      "\n",
      "\n",
      "Server Routes\n",
      "---------------\n",
      "Liveness Probe: GET   127.0.0.1:31311/\n",
      "Score:          POST  127.0.0.1:31311/score\n",
      "\n",
      "/bin/bash: /azureml-envs/azureml_f3f7e6c5fb83d94df23933000bf02da3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "2023-03-27T16:47:28,851813052+00:00 - gunicorn/finish 3 0\n",
      "2023-03-27T16:47:28,946299760+00:00 - Exit code 3 is not normal. Killing image.\n",
      "\n"
     ]
    },
    {
     "ename": "WebserviceException",
     "evalue": "WebserviceException:\n\tMessage: Error: Container has crashed. Did your init method fail?\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Error: Container has crashed. Did your init method fail?\"\n    }\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWebserviceException\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/10/5nbnv2696vlgb605wf3g6wwh0000gn/T/ipykernel_2419/3138792519.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m                        overwrite         = True)\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_deployment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshow_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/azureml/core/webservice/local.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                 raise WebserviceException('Cannot call {}() when service is {}.'.format(func.__name__, self.state),\n\u001b[1;32m     71\u001b[0m                                           logger=module_logger)\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/azureml/core/webservice/local.py\u001b[0m in \u001b[0;36mwait_for_deployment\u001b[0;34m(self, show_output)\u001b[0m\n\u001b[1;32m    609\u001b[0m         \"\"\"\n\u001b[1;32m    610\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m             container_health_check(self._port,\n\u001b[0m\u001b[1;32m    612\u001b[0m                                    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m                                    \u001b[0mhealth_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_health_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/azureml/_model_management/_util.py\u001b[0m in \u001b[0;36mcontainer_health_check\u001b[0;34m(docker_port, container, health_url, cleanup_if_failed)\u001b[0m\n\u001b[1;32m    746\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exited'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m             \u001b[0;31m# The container has started and crashed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m             _raise_for_container_failure(container, cleanup_if_failed,\n\u001b[0m\u001b[1;32m    749\u001b[0m                                          'Error: Container has crashed. Did your init method fail?')\n\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/azureml/_model_management/_util.py\u001b[0m in \u001b[0;36m_raise_for_container_failure\u001b[0;34m(container, cleanup, message)\u001b[0m\n\u001b[1;32m   1263\u001b[0m         \u001b[0mcleanup_container\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1265\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mWebserviceException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodule_logger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mWebserviceException\u001b[0m: WebserviceException:\n\tMessage: Error: Container has crashed. Did your init method fail?\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Error: Container has crashed. Did your init method fail?\"\n    }\n}"
     ]
    }
   ],
   "source": [
    "model = Model(ws, \"handmade_model\")\n",
    "\n",
    "service = Model.deploy(workspace         = ws,\n",
    "                       name              = \"handmade-predict\",\n",
    "                       models            = [model],\n",
    "                       inference_config  = dummy_inference_config,\n",
    "                       deployment_config = deployment_config,\n",
    "                       overwrite         = True)\n",
    "\n",
    "service.wait_for_deployment(show_output = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test is {'edad': 40.0, 'trabajo': 40.0, 'deuda': 40.0, 'saldo': 40.0, 'vivienda': 40.0, 'prestamo': 40.0, 'duracion': 40.0, 'fecha_contacto': 40.0, 'campaign': 40.0, 'tiempo_transcurrido': 40.0, 'contactos_anteriores': 40.0, 'target': 40.0, 'contactado': 40.0, 'desconocido': 40.0, 'fijo': 40.0, 'movil': 40.0, 'casado': 40.0, 'divorciado': 40.0, 'soltero': 40.0, 'exito': 40.0, 'nuevo_cliente': 40.0, 'otro': 40.0, 'sin_exito': 0, 'primaria': 1, 'secundaria/superiores': 0, 'universitarios': 40.0}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "uri = service.scoring_uri\n",
    "requests.get(\"http://localhost:6789\")\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "data = {\n",
    "        \"edad\": 40.0,\n",
    "        \"trabajo\": 40.0,\n",
    "        \"deuda\": 40.0,\n",
    "        \"saldo\": 40.0,\n",
    "        \"vivienda\": 40.0,\n",
    "        \"prestamo\": 40.0,\n",
    "        \"duracion\": 40.0,\n",
    "        \"fecha_contacto\": 40.0,\n",
    "        \"campaign\": 40.0,\n",
    "        \"tiempo_transcurrido\": 40.0,\n",
    "        \"contactos_anteriores\": 40.0,\n",
    "        \"target\": 40.0,\n",
    "        \"contactado\": 40.0,\n",
    "        \"desconocido\": 40.0,\n",
    "        \"fijo\": 40.0,\n",
    "        \"movil\": 40.0,\n",
    "        \"casado\": 40.0,\n",
    "        \"divorciado\": 40.0,\n",
    "        \"soltero\": 40.0,\n",
    "        \"exito\": 40.0,\n",
    "        \"nuevo_cliente\": 40.0,\n",
    "        \"otro\": 40.0,\n",
    "        \"sin_exito\": 0,\n",
    "        \"primaria\": 1,\n",
    "        \"secundaria/superiores\": 0,\n",
    "        \"universitarios\": 40.0\n",
    "      }\n",
    "\n",
    "data = json.dumps(data)\n",
    "response = requests.post(uri, data=data, headers=headers)\n",
    "print(response.json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
